{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import keras\n",
    "import pandas as pd\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Dropout, Activation, Embedding\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "# nltk.download()\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "# for human-friendly printing\n",
    "labels = ['fake', 'real']\n",
    "\n",
    "# read in our saved dictionary\n",
    "with open('./Dictionary_Models/keras_dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('./NN_Models/blstm_keras_embed_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "model.load_weights('./NN_Models/blstm_keras_embed_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a tweet to be evaluated: Allah please safe lest fire consume all of us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patrick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/patrick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real tweet\n",
      "\n",
      "Enter a tweet to be evaluated: Fire on the mountain coming for all of us because of our deeds!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patrick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/patrick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fake tweet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    #input a text string\n",
    "    text = input(\"Enter a tweet to be evaluated: \")\n",
    "    \n",
    "    # check if it is valid string\n",
    "    if len(text) == 0 or text == 'quit':\n",
    "        break\n",
    "        \n",
    "    tweet = text\n",
    "    \n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    #punctutation removal\n",
    "    def clean_text(text):\n",
    "        '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "        and remove words containing numbers.'''\n",
    "        text = text.lower()\n",
    "        text = re.sub('\\[.*?\\]', '', text)\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub('<.*?>+', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        return text\n",
    "    \n",
    "    # Tockenization\n",
    "    def tokenization(text):\n",
    "        text = re.split('\\W+', text)\n",
    "        return text\n",
    "\n",
    "    # stopword removal\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        text = [word for word in text if word not in stopword]\n",
    "        return text\n",
    "\n",
    "    # return to string\n",
    "    def listToString(s):     \n",
    "        # initialize an empty string \n",
    "        str1 = \" \" \n",
    "        # return string   \n",
    "        return (str1.join(s))\n",
    "    \n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "    porter =  PorterStemmer()\n",
    "\n",
    "    def sentenceStemmer(text):\n",
    "        words = word_tokenize(text)\n",
    "        corpus = []\n",
    "        for word in words:\n",
    "            corpus.append(porter.stem(word))\n",
    "            corpus.append(\" \")\n",
    "        #end for\n",
    "        return \"\".join(corpus)\n",
    "    #end sentenceStemmer\n",
    "    \n",
    "    def convert_text_to_index_array(text):\n",
    "        words = kpt.text_to_word_sequence(text)\n",
    "        # words = pad_sequences(words, padding='post', maxlen=23)\n",
    "        # print(text)\n",
    "        wordIndices = []\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                wordIndices.append(dictionary[word])\n",
    "        return wordIndices\n",
    "    \n",
    "    # Clean the text\n",
    "    text = clean_text(text)\n",
    "    text = tokenization(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = listToString(text)\n",
    "    text = sentenceStemmer(text)\n",
    "    \n",
    "    # evaluating the text\n",
    "    text = convert_text_to_index_array(text)\n",
    "\n",
    "    padded_text = pad_sequences([text], padding='post', maxlen=100)\n",
    "    \n",
    "    pred = model.predict_generator(padded_text)\n",
    "\n",
    "    def rounding(results):\n",
    "        '''Results needs to be rounded to 0 or 1 for fake or real, respectively'''\n",
    "        if results < 0.5:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    predictions_final = [rounding(x) for x in pred]\n",
    "    \n",
    "    if predictions_final[0] == 0:\n",
    "        print(\"\\nFake tweet\\n\")\n",
    "    if predictions_final[0] == 1:\n",
    "        print(\"\\nReal tweet\\n\")\n",
    "\n",
    "#end while    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweepy Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
