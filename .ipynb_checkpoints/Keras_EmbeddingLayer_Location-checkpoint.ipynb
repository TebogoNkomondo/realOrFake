{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import pandas as pd\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Dropout, Activation, Embedding, Flatten\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "# nltk.download()\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                               Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n",
       "3                                    13,000 people receive #wildfires evacuation orders in California    \n",
       "4             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('nlp-getting-started/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5080.000000</td>\n",
       "      <td>5080</td>\n",
       "      <td>5080</td>\n",
       "      <td>5080</td>\n",
       "      <td>5080.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>221</td>\n",
       "      <td>3341</td>\n",
       "      <td>5028</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>collision</td>\n",
       "      <td>USA</td>\n",
       "      <td>#Bestnaijamade: 16yr old PKK suicide bomber who detonated bomb in ... http://t.co/KSAwlYuX02 bes...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>104</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5407.112598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.432283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3116.359041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.495442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2728.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5360.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8086.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10833.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id    keyword location  \\\n",
       "count    5080.000000       5080     5080   \n",
       "unique           NaN        221     3341   \n",
       "top              NaN  collision      USA   \n",
       "freq             NaN         36      104   \n",
       "mean     5407.112598        NaN      NaN   \n",
       "std      3116.359041        NaN      NaN   \n",
       "min        48.000000        NaN      NaN   \n",
       "25%      2728.750000        NaN      NaN   \n",
       "50%      5360.500000        NaN      NaN   \n",
       "75%      8086.000000        NaN      NaN   \n",
       "max     10833.000000        NaN      NaN   \n",
       "\n",
       "                                                                                                       text  \\\n",
       "count                                                                                                  5080   \n",
       "unique                                                                                                 5028   \n",
       "top     #Bestnaijamade: 16yr old PKK suicide bomber who detonated bomb in ... http://t.co/KSAwlYuX02 bes...   \n",
       "freq                                                                                                      6   \n",
       "mean                                                                                                    NaN   \n",
       "std                                                                                                     NaN   \n",
       "min                                                                                                     NaN   \n",
       "25%                                                                                                     NaN   \n",
       "50%                                                                                                     NaN   \n",
       "75%                                                                                                     NaN   \n",
       "max                                                                                                     NaN   \n",
       "\n",
       "             target  \n",
       "count   5080.000000  \n",
       "unique          NaN  \n",
       "top             NaN  \n",
       "freq            NaN  \n",
       "mean       0.432283  \n",
       "std        0.495442  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_after_null_removal = data.copy()\n",
    "data_after_null_removal = data_after_null_removal.dropna(subset=['location'])\n",
    "data_after_null_removal.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C</td>\n",
       "      <td>1</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C Birmingham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw</td>\n",
       "      <td>0</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw Est. September 2012 - Bristol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi</td>\n",
       "      <td>1</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>Crying out for more! Set me ablaze Philadelphia, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N</td>\n",
       "      <td>0</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N London, UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>10826</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>TN</td>\n",
       "      <td>On the bright side I wrecked http://t.co/uEa0txRHYs</td>\n",
       "      <td>0</td>\n",
       "      <td>On the bright side I wrecked http://t.co/uEa0txRHYs TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>10829</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>#NewcastleuponTyne #UK</td>\n",
       "      <td>@widda16 ... He's gone. You can relax. I thought the wife who wrecked her cake was a goner mind ...</td>\n",
       "      <td>0</td>\n",
       "      <td>@widda16 ... He's gone. You can relax. I thought the wife who wrecked her cake was a goner mind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty much all been wrecked hahaha shoutout to my family f...</td>\n",
       "      <td>0</td>\n",
       "      <td>Three days off from work and they've pretty much all been wrecked hahaha shoutout to my family f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/7enNulLKzM</td>\n",
       "      <td>0</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/7enNulLKzM Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Lion gig tonight. Hearing is wrecked. http://t.co/oM...</td>\n",
       "      <td>0</td>\n",
       "      <td>@engineshed Great atmosphere at the British Lion gig tonight. Hearing is wrecked. http://t.co/oM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5080 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                       location  \\\n",
       "31       48   ablaze                     Birmingham   \n",
       "32       49   ablaze  Est. September 2012 - Bristol   \n",
       "33       50   ablaze                         AFRICA   \n",
       "34       52   ablaze               Philadelphia, PA   \n",
       "35       53   ablaze                     London, UK   \n",
       "...     ...      ...                            ...   \n",
       "7575  10826  wrecked                             TN   \n",
       "7577  10829  wrecked         #NewcastleuponTyne #UK   \n",
       "7579  10831  wrecked              Vancouver, Canada   \n",
       "7580  10832  wrecked                        London    \n",
       "7581  10833  wrecked                        Lincoln   \n",
       "\n",
       "                                                                                                     text  \\\n",
       "31                                                @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C   \n",
       "32                                    We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw   \n",
       "33                     #AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi   \n",
       "34                                                                     Crying out for more! Set me ablaze   \n",
       "35                           On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N   \n",
       "...                                                                                                   ...   \n",
       "7575                                                  On the bright side I wrecked http://t.co/uEa0txRHYs   \n",
       "7577  @widda16 ... He's gone. You can relax. I thought the wife who wrecked her cake was a goner mind ...   \n",
       "7579  Three days off from work and they've pretty much all been wrecked hahaha shoutout to my family f...   \n",
       "7580        #FX #forex #trading Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/7enNulLKzM   \n",
       "7581  @engineshed Great atmosphere at the British Lion gig tonight. Hearing is wrecked. http://t.co/oM...   \n",
       "\n",
       "      target  \\\n",
       "31         1   \n",
       "32         0   \n",
       "33         1   \n",
       "34         0   \n",
       "35         0   \n",
       "...      ...   \n",
       "7575       0   \n",
       "7577       0   \n",
       "7579       0   \n",
       "7580       0   \n",
       "7581       0   \n",
       "\n",
       "                                                                                                   tweets  \n",
       "31                                     @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C Birmingham  \n",
       "32      We always try to bring the heavy. #metal #RT http://t.co/YAo1e0xngw Est. September 2012 - Bristol  \n",
       "33              #AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi AFRICA  \n",
       "34                                                    Crying out for more! Set me ablaze Philadelphia, PA  \n",
       "35                On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N London, UK  \n",
       "...                                                                                                   ...  \n",
       "7575                                               On the bright side I wrecked http://t.co/uEa0txRHYs TN  \n",
       "7577  @widda16 ... He's gone. You can relax. I thought the wife who wrecked her cake was a goner mind ...  \n",
       "7579  Three days off from work and they've pretty much all been wrecked hahaha shoutout to my family f...  \n",
       "7580  #FX #forex #trading Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/7enNulLKzM Lo...  \n",
       "7581  @engineshed Great atmosphere at the British Lion gig tonight. Hearing is wrecked. http://t.co/oM...  \n",
       "\n",
       "[5080 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Location + texts\n",
    "data_after_adding_location = data_after_null_removal.copy()\n",
    "\n",
    "data_after_adding_location['tweets'] = data_after_adding_location['text'].str.cat(data_after_adding_location['location'],sep=\" \")\n",
    "data_after_adding_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "#punctutation removal\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "data_afer_punctuation_removal = data_after_adding_location.copy()\n",
    "data_afer_punctuation_removal['tweets'] = data_afer_punctuation_removal['tweets'].apply(lambda x: remove_punct(x))\n",
    "data_afer_punctuation_removal.head(10)\n",
    "\n",
    "# Tockenization\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data_afer_punctuation_removal.copy()\n",
    "tockenized_data['tweets'] = tockenized_data['tweets'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "# stopword removal \n",
    "data_after_stopword_removal = tockenized_data.copy()\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data_after_stopword_removal['tweets'] = data_after_stopword_removal['tweets'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# return to string\n",
    "data_without_tockenization = data_after_stopword_removal.copy()\n",
    "def listToString(s):     \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "        \n",
    "data_without_tockenization['tweets'] = data_without_tockenization['tweets'].apply(lambda x: listToString(x))\n",
    "data_without_tockenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train - test split\n",
    "training, testing = train_test_split(data_without_tockenization, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training.copy()\n",
    "test = testing.copy()\n",
    "# get the dependent and independent variables\n",
    "train_x = train['tweets']\n",
    "train_y = train['target']\n",
    "test_x = test['tweets']\n",
    "test_y = test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "\n",
    "# only work with the 10000 most popular words found in our dataset\n",
    "max_words = 30000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "# Saving the dictionary\n",
    "with open('./Dictionary_Models/locdictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "# Function to convert the text to its corresponding index\n",
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "############################\n",
    "\n",
    "# Embedding Representation\n",
    "\n",
    "############################\n",
    "# Padding each text so that it has the same length\n",
    "text_length = 30\n",
    "embedded_text = pad_sequences(allWordIndices, padding='pre', maxlen=text_length)\n",
    "\n",
    "train_x = np.asarray(embedded_text)\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "embedding_features = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words,output_dim=embedding_features, input_length=text_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('./NN_Models/embed_keras_loc_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('./NN_Models/embed_keras_loc_model.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['fake', 'real']\n",
    "\n",
    "# read in our saved dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "\n",
    "    return wordIndices\n",
    "\n",
    "# read in your saved model structure\n",
    "json_file = open('embeddingModel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('embeddingModel.h5')\n",
    "\n",
    "text_sequence = []\n",
    "\n",
    "for index_of_interest, text1 in enumerate(test_x):\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(text1)\n",
    "    text_sequence.append(testArr)\n",
    "    \n",
    "padded_sequence = pad_sequences(text_sequence, padding='pre', maxlen=text_length)\n",
    "predictions_before_formatting = model.predict(padded_sequence)\n",
    "\n",
    "predictions = []\n",
    "for num in predictions_before_formatting:\n",
    "  predictions.append(np.argmax(num))\n",
    "#end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding(results):\n",
    "    '''Results needs to be rounded to 0 or 1 for fake or real, respectively'''\n",
    "    if results < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "predictions_final = [rounding(x) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "test_y = np.array(test_y)\n",
    "confusion_matrix(test_y, predictions)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, predictions).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tn + fp +fn +tp)\n",
    "falsePositiveRate = fp / (fp + tn)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2 * ((recall*precision)/(recall+precision))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d = {'Value' : pd.Series([accuracy, precision, falsePositiveRate, recall,f1_score], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results = pd.DataFrame(d) \n",
    "  \n",
    "\n",
    "data = {'Negative':[tn, fn], 'Positive':[fp, tp]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df = pd.DataFrame(data, index =['Negative', 'Positive']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- After running the model few times, it was observed that the inclusion of the location features does affect the overall performance of the model. \n",
    "- Hence, it is better to not include it at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
