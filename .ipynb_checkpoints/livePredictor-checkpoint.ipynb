{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import keras\n",
    "import pandas as pd\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Dropout, Activation, Embedding\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "# nltk.download()\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=3000)\n",
    "# for human-friendly printing\n",
    "labels = ['fake', 'real']\n",
    "\n",
    "# read in our saved dictionary\n",
    "with open('./Dictionary_Models/word2vec_models_dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this utility makes sure that all the words in your input\n",
    "# are registered in the dictionary\n",
    "# before trying to turn them into a matrix.\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        # else:\n",
    "        #     print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your saved model structure\n",
    "json_file = open('./NN_Models/word2vec_BLSTM_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('./NN_Models/word2vec_BLSTM_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a sentence to be evaluated, or Enter to quit:there is a fire in the mountain\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      " sentiment:  fake\n",
      "Input a sentence to be evaluated, or Enter to quit:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while 1:\n",
    "    evalSentence =input('Input a sentence to be evaluated, or Enter to quit:')\n",
    "\n",
    "    if len(evalSentence) == 0:\n",
    "        break\n",
    "        \n",
    "\n",
    "    #### removing entries where location values are missing\n",
    "#     data_after_null_removal = evalSentence\n",
    "\n",
    "    #punctutation removal\n",
    "    def clean_text(text):\n",
    "        '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "        and remove words containing numbers.'''\n",
    "        text = text.lower()\n",
    "        text = re.sub('\\[.*?\\]', '', text)\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub('<.*?>+', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        return text\n",
    "\n",
    "#     data_afer_punctuation_removal = data_after_null_removal.copy()\n",
    "#     data_afer_punctuation_removal['text'] = data_afer_punctuation_removal['text'].apply(lambda x: clean_text(x))\n",
    "#     data_afer_punctuation_removal.head(10)\n",
    "    \n",
    "    data_afer_punctuation_removal = clean_text(evalSentence)\n",
    "\n",
    "    # Tockenization\n",
    "\n",
    "    def tokenization(text):\n",
    "        text = re.split('\\W+', text)\n",
    "        return text\n",
    "\n",
    "#     tockenized_data = data_afer_punctuation_removal.copy()\n",
    "#     tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "    tockenized_data = tokenization(data_afer_punctuation_removal)\n",
    "\n",
    "    # stopword removal \n",
    "#     data_after_stopword_removal = tockenized_data.copy()\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        text = [word for word in text if word not in stopword]\n",
    "        return text\n",
    "\n",
    "#     data_after_stopword_removal['text'] = data_after_stopword_removal['text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    data_after_stopword_removal = remove_stopwords(tockenized_data)\n",
    "    \n",
    "    # return to string\n",
    "#     data_without_tockenization = data_after_stopword_removal.copy()\n",
    "    def listToString(s):     \n",
    "        # initialize an empty string \n",
    "        str1 = \" \" \n",
    "        # return string   \n",
    "        return (str1.join(s)) \n",
    "\n",
    "#     data_without_tockenization['text'] = data_without_tockenization['text'].apply(lambda x: listToString(x))\n",
    "    data_without_tockenization = listToString(data_after_stopword_removal)\n",
    "        \n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=3000)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(data_without_tockenization)\n",
    "    \n",
    "    max_length = 23\n",
    "    X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "        \n",
    "        \n",
    "#     max_length = 23\n",
    "#     evalT = tokenizer.texts_to_sequences(data_without_tockenization)\n",
    "#     print(evalT)\n",
    "#     evalT = pad_sequences(evalT, maxlen=max_length, padding='post')\n",
    "#     print(evalT)\n",
    "\n",
    "    print(X_train)\n",
    "    \n",
    "    pred = model.predict(X_train)\n",
    " \n",
    "    print(' sentiment: ', labels[np.argmax(pred)])\n",
    "    \n",
    "\n",
    "    \n",
    "#     testArr = convert_text_to_index_array(evalSentence)\n",
    "#     input = tokenizer.sequences_to_matrix(testArr, mode='binary')\n",
    "#     # predict which bucket your input belongs in\n",
    "\n",
    "# #     format your input for the neural net\n",
    "#     testArr = convert_text_to_index_array(text1)\n",
    "#     input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    \n",
    "#     pred = model.predict(input)\n",
    " \n",
    "#     print(' sentiment: ', labels[np.argmax(pred)])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
