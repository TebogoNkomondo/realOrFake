{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kELKzweMZaU"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WX4N1L4sMc7z"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROkgT8ELNxwZ"
   },
   "outputs": [],
   "source": [
    "downloaded = drive.CreateFile({'id':'1-Hh3pdJlxhHWy42ZqXFDdjjBxe4vhoRg'})\n",
    "downloaded.GetContentFile('train.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shGyaP_zytVP"
   },
   "source": [
    "# Base Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-xRtolszHr5"
   },
   "source": [
    "## Importing relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isru0U_VzLu2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import pandas as pd\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Dropout, Activation, Embedding\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "# nltk.download()\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEfaIuWN3KT2"
   },
   "source": [
    "## Exploring data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "VfuxUgGt3RAD",
    "outputId": "d5d40978-95d0-42e5-fa33-594c295c2acc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                               Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n",
       "3                                    13,000 people receive #wildfires evacuation orders in California    \n",
       "4             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./nlp-getting-started/train.csv')\n",
    "# Only alter the training variable (#never alter the data variable itself)\n",
    "# training = data\n",
    "# # split the data into train and test set\n",
    "# train, test = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkmCYdOXKPiM"
   },
   "source": [
    "### Analysis\n",
    "- there are 7613 data points\n",
    "- **99.198739%** of the data has **keywords**\n",
    "- **66.73%** of the data has **location** points\n",
    "- the top key word used to extract tweets is **fatalities**\n",
    "- data is ordered in terms of keyword used to extact the tweet from twitter \n",
    "- therefore shuffle the data to mix it.\n",
    "- some of the data contains the # symbol which causes an error when the data is exported onto a numpy array\n",
    "- elements in the **text** column which does not have \" \" marks should not include **,** \n",
    "- data in the **location** column may also include **,** marks which will be read as a column delimeter by **np**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnpG9wg1Kaqe"
   },
   "source": [
    "### Decisions \n",
    "\n",
    "- the most important columns are the text and target columns\n",
    "- the text column contains information about the tweet\n",
    "- the keyword column can be discarded because the keyword appears within the tweet itself.\n",
    "- the location column can be discarded because only 66.73% have a location value associated with them. Droping 33% of the data is impractical\n",
    "- it is however worth exploring whether location of tweet has an impact on the real or fake status of a tweet\n",
    "\n",
    "- in some locations such as a city centre there cannot be a veld fire - so that is a consideration to be made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBgMp8HCJhxe"
   },
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIvSjP1PJfUQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officers evacuation shelter place orders expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding bridge collapse nearby homes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrary thetawniest control wild fires california even northern part state troubling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating ebike collided car little portugal ebike rider suffered serious nonlife thr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest homes razed northern california wildfire abc news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                                                                     text  \\\n",
       "0                                                            deeds reason earthquake may allah forgive us   \n",
       "1                                                                   forest fire near la ronge sask canada   \n",
       "2                residents asked shelter place notified officers evacuation shelter place orders expected   \n",
       "3                                                  people receive wildfires evacuation orders california    \n",
       "4                                                got sent photo ruby alaska smoke wildfires pours school    \n",
       "...                                                                                                   ...   \n",
       "7608                                               two giant cranes holding bridge collapse nearby homes    \n",
       "7609              ariaahrary thetawniest control wild fires california even northern part state troubling   \n",
       "7610                                                                                      volcano hawaii    \n",
       "7611  police investigating ebike collided car little portugal ebike rider suffered serious nonlife thr...   \n",
       "7612                                            latest homes razed northern california wildfire abc news    \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### removing entries where location values are missing\n",
    "data_after_null_removal = data.copy()\n",
    "# data_after_null_removal = data_after_null_removal.dropna(subset=['location'])\n",
    "\n",
    "#punctutation removal\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "data_afer_punctuation_removal = data_after_null_removal.copy()\n",
    "data_afer_punctuation_removal['text'] = data_afer_punctuation_removal['text'].apply(lambda x: clean_text(x))\n",
    "data_afer_punctuation_removal.head(10)\n",
    "\n",
    "# Tockenization\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data_afer_punctuation_removal.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "# stopword removal \n",
    "data_after_stopword_removal = tockenized_data.copy()\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data_after_stopword_removal['text'] = data_after_stopword_removal['text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# return to string\n",
    "data_without_tockenization = data_after_stopword_removal.copy()\n",
    "def listToString(s):     \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "        \n",
    "data_without_tockenization['text'] = data_without_tockenization['text'].apply(lambda x: listToString(x))\n",
    "data_without_tockenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAJ5_I2wLS2w"
   },
   "outputs": [],
   "source": [
    "### train - test split\n",
    "training, testing = train_test_split(data_without_tockenization, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObaL-amMDiEt"
   },
   "outputs": [],
   "source": [
    "train = training.copy()\n",
    "test = testing.copy()\n",
    "# get the dependent and independent variables\n",
    "train_x = train['text']\n",
    "train_y = train['target']\n",
    "test_x = test['text']\n",
    "test_y = test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4JIjdRbvw5f"
   },
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_-YWhL2vzi0"
   },
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "\n",
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 10000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "# Saving the dictionary\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "# padding tockenized text so that it is all the same length(longest word's length).\n",
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "#array of tweets as indeces(words replaced with indexes)\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gewOSOgE0mWW"
   },
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "wYv4KThy0oT6",
    "outputId": "9c6ba703-40cb-4c38-d3d2-5127f21c60ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "386/386 [==============================] - 17s 44ms/step - loss: 0.5378 - accuracy: 0.7315 - precision: 0.7315 - recall: 0.7315 - val_loss: 0.4592 - val_accuracy: 0.7857 - val_precision: 0.7857 - val_recall: 0.7857\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - 15s 39ms/step - loss: 0.2551 - accuracy: 0.9030 - precision: 0.9030 - recall: 0.9030 - val_loss: 0.5378 - val_accuracy: 0.7624 - val_precision: 0.7624 - val_recall: 0.7624\n",
      "Epoch 00002: early stopping\n",
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(512, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','Precision','Recall'])\n",
    "\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "ERhhWVV23Ehl",
    "outputId": "a8521166-3566-4400-e0d7-9d0d1ae38b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdShkFnuJWnN"
   },
   "source": [
    "## Testing Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1faniBdPTZXz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "# for human-friendly printing\n",
    "labels = ['fake', 'real']\n",
    "\n",
    "# read in our saved dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "\n",
    "    return wordIndices\n",
    "\n",
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "f = open(\"output.txt\",\"w\")\n",
    "for index_of_interest, text1 in enumerate(test_x):\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(text1)\n",
    "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    # predict which bucket your input belongs in\n",
    "\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(text1)\n",
    "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        # predict which bucket your input belongs in\n",
    "    pred = model.predict(input)\n",
    "\n",
    "    predictions.append(np.argmax(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mGyqH1Npducg",
    "outputId": "3417db05-3167-43d7-eada-9d28807c725e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>345</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>95</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Positive\n",
       "Negative       345        81\n",
       "Positive        95       241"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = np.array(predictions)\n",
    "test_y = np.array(test_y)\n",
    "confusion_matrix(test_y, predictions)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, predictions).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tn + fp +fn +tp)\n",
    "falsePositiveRate = fp / (fp + tn)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2 * ((recall*precision)/(recall+precision))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d = {'Value' : pd.Series([accuracy, precision, falsePositiveRate, recall,f1_score], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results = pd.DataFrame(d) \n",
    "  \n",
    "\n",
    "data = {'Negative':[tn, fn], 'Positive':[fp, tp]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df = pd.DataFrame(data, index =['Negative', 'Positive']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.769029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.748447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>0.190141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.717262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.732523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "Accuracy   0.769029\n",
       "Precision  0.748447\n",
       "FPR        0.190141\n",
       "Recall     0.717262\n",
       "F1         0.732523"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDpDt1Vw4hmt"
   },
   "source": [
    "### Summary of Base model\n",
    "- The highest achievable accuracy is 77%\n",
    "- The best FPR is 0.09\n",
    "- Model starts overfitting from the first epoch\n",
    "- Validation loss never goes below ~0.45\n",
    "- Attempts were made to vary the batch size. Large batch sizes yield worse performance. The best performance is at 32 or 16\n",
    "- Increasing the number of layers also results in worse performance. Less layers produce better performance\n",
    "- Drop out layers make little to no difference in performance\n",
    "- Including tweets without the keyword seems to yield better performance\n",
    "- Data cleaning produced a slightly less validation loss but not significant enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-trained Word2vec using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data_without_tockenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize data\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data2.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "tockenized_data.head()\n",
    "\n",
    "tweet_data = tockenized_data['text']\n",
    "tweet_data[1]\n",
    "\n",
    "tweet_data_array = []\n",
    "# store tweets in array for word2vec\n",
    "for arr in tweet_data:\n",
    "    tweet_data_array.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training own word2vec model\n",
    "vector_size = 512\n",
    "window_size = 10\n",
    "# Create Word2Vec\n",
    "word2vec = Word2Vec(sentences=tweet_data_array,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "word2vec.save('ownWord2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "word2vec = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f69381eb0a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word vector\n",
    "X_vecs = word2vec.wv\n",
    "\n",
    "X_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 9.545514251937476\n",
      "Max tweet length: 23\n"
     ]
    }
   ],
   "source": [
    "# vector size\n",
    "test_size = int(0.1*len(tweet_data))\n",
    "train_size = len(tweet_data) - test_size\n",
    "\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tweet_data:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tweet_data))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "max_tweet_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "### train - test split\n",
    "training, testing = train_test_split(tockenized_data, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "x_train = training['text']\n",
    "y_train = training['target']\n",
    "\n",
    "x_test = testing['text']\n",
    "y_test = testing['target']\n",
    "\n",
    "# variables for storing word2vec representations of tweets\n",
    "X_train = np.zeros((len(training), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((len(training), 2), dtype=np.int32)\n",
    "X_test = np.zeros((len(testing), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "# converting tweets(training set) to word2vec representations\n",
    "train_y = keras.utils.to_categorical(y_train, 2)\n",
    "t=0\n",
    "for index, tweet in enumerate(x_train):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs:\n",
    "            continue\n",
    "        else:\n",
    "            X_train[index, t, :] = X_vecs[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 15, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweets(testing set) to word2vec representations\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "t=0\n",
    "for index, tweet in enumerate(x_test):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs:\n",
    "            continue\n",
    "        else:\n",
    "            X_test[index, t, :] = X_vecs[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "#             print(test_y['target'] ,'\\n')\n",
    "            if test_y[index] == 1:\n",
    "                 Y_test[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_test[index, :] = [0.0, 1.0] \n",
    "#             Y_test[index, :] = [1.0, 0.0] if test_y[1] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f2b8d3086410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating a model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedding_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating a model\n",
    "embedding_features = 16\n",
    "model = Sequential()\n",
    "model.add(word2vec.wv.get_keras_embedding(train_embeddings=False))\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('LSTM1_EmbeddingModel.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('LSTM1_EmbeddingModel.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training own Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data_without_tockenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data2.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "tockenized_data.head()\n",
    "\n",
    "tweet_data = tockenized_data['text']\n",
    "tweet_data[1]\n",
    "\n",
    "tweet_data_array = []\n",
    "\n",
    "for arr in tweet_data:\n",
    "    tweet_data_array.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 512\n",
    "window_size = 10\n",
    "# Create Word2Vec\n",
    "word2vec_self_trained = Word2Vec(sentences=tweet_data_array,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_self_trained.save('word2vecSelfTrained.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_self_trained = Word2Vec.load('word2vecSelfTrained.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fc313ee72b0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vecs_self_trained = word2vec_self_trained.wv\n",
    "\n",
    "X_vecs_self_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 9.545514251937476\n",
      "Max tweet length: 23\n"
     ]
    }
   ],
   "source": [
    "test_size = int(0.1*len(tweet_data))\n",
    "train_size = len(tweet_data) - test_size\n",
    "\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tweet_data:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tweet_data))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "max_tweet_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "### train - test split\n",
    "training, testing = train_test_split(tockenized_data, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "training\n",
    "\n",
    "x_train = training['text']\n",
    "y_train = training['target']\n",
    "\n",
    "x_test = testing['text']\n",
    "y_test = testing['target']\n",
    "\n",
    "\n",
    "X_train = np.zeros((len(training), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((len(training), 2), dtype=np.int32)\n",
    "X_test = np.zeros((len(testing), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "train_y = np.array(y_train)\n",
    "\n",
    "t=0\n",
    "for index, tweet in enumerate(x_train):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs_self_trained:\n",
    "            continue\n",
    "        else:\n",
    "            X_train[index, t, :] = X_vecs_self_trained[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "                \n",
    "            if train_y[index] == 1:\n",
    "                 Y_train[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_train[index, :] = [0.0, 1.0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "for index, tweet in enumerate(x_test):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs_self_trained:\n",
    "            continue\n",
    "        else:\n",
    "            X_test[index, t, :] = X_vecs_self_trained[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "#             print(test_y['target'] ,'\\n')\n",
    "            if test_y[index] == 1:\n",
    "                 Y_test[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_test[index, :] = [0.0, 1.0] \n",
    "#             Y_test[index, :] = [1.0, 0.0] if test_y[1] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = keras.utils.to_categorical(y_train, 2)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','Precision','Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "386/386 [==============================] - 80s 207ms/step - loss: 0.5158 - accuracy: 0.7607 - precision: 0.7607 - recall: 0.7620 - val_loss: 0.5223 - val_accuracy: 0.7595 - val_precision: 0.7595 - val_recall: 0.7595\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - 78s 203ms/step - loss: 0.4568 - accuracy: 0.7966 - precision: 0.7966 - recall: 0.7979 - val_loss: 0.4852 - val_accuracy: 0.7843 - val_precision: 0.7843 - val_recall: 0.7843\n",
      "Epoch 3/10\n",
      "386/386 [==============================] - 76s 196ms/step - loss: 0.4277 - accuracy: 0.8075 - precision: 0.8075 - recall: 0.8088 - val_loss: 0.4708 - val_accuracy: 0.7930 - val_precision: 0.7930 - val_recall: 0.7930\n",
      "Epoch 4/10\n",
      "386/386 [==============================] - 75s 195ms/step - loss: 0.3865 - accuracy: 0.8294 - precision: 0.8294 - recall: 0.8307 - val_loss: 0.5147 - val_accuracy: 0.7799 - val_precision: 0.7799 - val_recall: 0.7799\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc31640a190>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 15, 512)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 3s 61ms/step - loss: 0.5345 - accuracy: 0.7782 - precision: 0.7782 - recall: 0.7803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5345075726509094, 0.778215229511261, 0.778215229511261, 0.7802631855010986]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lstm_word2vec_self_trained = []\n",
    "\n",
    "for ind, lab in Y_test:\n",
    "    if Y_test[ind][0] == 0:\n",
    "        y_test_lstm_word2vec_self_trained.append(0)\n",
    "    else:\n",
    "        y_test_lstm_word2vec_self_trained.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>229</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>63</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Positive\n",
       "Negative       229       107\n",
       "Positive        63       363"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "Y_pred = model.predict_generator(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "tn_lstm_word2vec_self_trained, fp_lstm_word2vec_self_trained, fn_lstm_word2vec_self_trained, tp_lstm_word2vec_self_trained = confusion_matrix(y_test_lstm_word2vec_self_trained, y_pred).ravel()\n",
    "precision_lstm_word2vec_self_trained = tp_lstm_word2vec_self_trained / (tp_lstm_word2vec_self_trained + fp_lstm_word2vec_self_trained)\n",
    "accuracy_lstm_word2vec_self_trained = (tp_lstm_word2vec_self_trained + tn_lstm_word2vec_self_trained) / (tn_lstm_word2vec_self_trained + fp_lstm_word2vec_self_trained +fn_lstm_word2vec_self_trained +tp_lstm_word2vec_self_trained)\n",
    "falsePositiveRate_lstm_word2vec_self_trained = fp_lstm_word2vec_self_trained / (fp_lstm_word2vec_self_trained + tn_lstm_word2vec_self_trained)\n",
    "recall_lstm_word2vec_self_trained = tp_lstm_word2vec_self_trained/(tp_lstm_word2vec_self_trained+fn_lstm_word2vec_self_trained)\n",
    "f1_score_lstm_word2vec_self_trained = 2 * ((recall_lstm_word2vec_self_trained*precision_lstm_word2vec_self_trained)/(recall_lstm_word2vec_self_trained+precision_lstm_word2vec_self_trained))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d = {'Value' : pd.Series([accuracy_lstm_word2vec_self_trained, precision_lstm_word2vec_self_trained, falsePositiveRate_lstm_word2vec_self_trained, recall_lstm_word2vec_self_trained,f1_score_lstm_word2vec_self_trained], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results_lstm_word2vec_self_trained = pd.DataFrame(d) \n",
    "  \n",
    "\n",
    "data = {'Negative':[tn_lstm_word2vec_self_trained, fn_lstm_word2vec_self_trained], 'Positive':[fp_lstm_word2vec_self_trained, tp_lstm_word2vec_self_trained]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df_lstm_word2vec_self_trained = pd.DataFrame(data, index =['Negative', 'Positive']) \n",
    "df_lstm_word2vec_self_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.776903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.772340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>0.318452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.852113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.810268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "Accuracy   0.776903\n",
       "Precision  0.772340\n",
       "FPR        0.318452\n",
       "Recall     0.852113\n",
       "F1         0.810268"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lstm_word2vec_self_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "pretrainedModel = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.save('pretrainedModel.model')\n",
    "word2vec_pretrained = Word2Vec.load('pretrainedModel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fc3175c5a00>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vecs_pretrained = word2vec_pretrained.wv\n",
    "\n",
    "X_vecs_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 9.545514251937476\n",
      "Max tweet length: 23\n"
     ]
    }
   ],
   "source": [
    "test_size = int(0.1*len(tweet_data))\n",
    "train_size = len(tweet_data) - test_size\n",
    "\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tweet_data:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tweet_data))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "max_tweet_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "### train - test split\n",
    "training, testing = train_test_split(tockenized_data, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "training\n",
    "\n",
    "x_train = training['text']\n",
    "y_train = training['target']\n",
    "\n",
    "x_test = testing['text']\n",
    "y_test = testing['target']\n",
    "\n",
    "\n",
    "X_train = np.zeros((len(training), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((len(training), 2), dtype=np.int32)\n",
    "X_test = np.zeros((len(testing), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "train_y = np.array(y_train)\n",
    "\n",
    "t=0\n",
    "for index, tweet in enumerate(x_train):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs_pretrained:\n",
    "            continue\n",
    "        else:\n",
    "            X_train[index, t, :] = X_vecs_pretrained[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "                \n",
    "            if train_y[index] == 1:\n",
    "                 Y_train[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_train[index, :] = [0.0, 1.0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(y_test)\n",
    "t=0\n",
    "for index, tweet in enumerate(x_test):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs_pretrained:\n",
    "            continue\n",
    "        else:\n",
    "            X_test[index, t, :] = X_vecs_pretrained[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "#             print(test_y['target'] ,'\\n')\n",
    "            if test_y[index] == 1:\n",
    "                 Y_test[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_test[index, :] = [0.0, 1.0] \n",
    "#             Y_test[index, :] = [1.0, 0.0] if test_y[1] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = keras.utils.to_categorical(y_train, 2)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','Precision','Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "386/386 [==============================] - 71s 183ms/step - loss: 0.5173 - accuracy: 0.7606 - precision: 0.7606 - recall: 0.7618 - val_loss: 0.5092 - val_accuracy: 0.7726 - val_precision: 0.7726 - val_recall: 0.7726\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - 60s 154ms/step - loss: 0.4646 - accuracy: 0.7917 - precision: 0.7917 - recall: 0.7930 - val_loss: 0.4824 - val_accuracy: 0.7682 - val_precision: 0.7682 - val_recall: 0.7682\n",
      "Epoch 3/10\n",
      "386/386 [==============================] - 58s 151ms/step - loss: 0.4226 - accuracy: 0.8112 - precision: 0.8112 - recall: 0.8125 - val_loss: 0.4704 - val_accuracy: 0.8047 - val_precision: 0.8047 - val_recall: 0.8047\n",
      "Epoch 4/10\n",
      "386/386 [==============================] - 59s 152ms/step - loss: 0.3903 - accuracy: 0.8284 - precision: 0.8284 - recall: 0.8297 - val_loss: 0.4882 - val_accuracy: 0.7843 - val_precision: 0.7843 - val_recall: 0.7843\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc30142d700>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 3s 56ms/step - loss: 0.5243 - accuracy: 0.7743 - precision: 0.7743 - recall: 0.7763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5243053436279297,\n",
       " 0.7742782235145569,\n",
       " 0.7742782235145569,\n",
       " 0.7763158082962036]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lstm_word2vec_pretrained = []\n",
    "\n",
    "for ind, lab in Y_test:\n",
    "    if Y_test[ind][0] == 0:\n",
    "        y_test_lstm_word2vec_pretrained.append(0)\n",
    "    else:\n",
    "        y_test_lstm_word2vec_pretrained.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>229</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>63</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Positive\n",
       "Negative       229       107\n",
       "Positive        63       363"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "Y_pred = model.predict_generator(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "tn_lstm_word2vec_pretrained, fp_lstm_word2vec_pretrained, fn_lstm_word2vec_pretrained, tp_lstm_word2vec_pretrained = confusion_matrix(y_test_lstm_word2vec_pretrained, y_pred).ravel()\n",
    "precision_lstm_word2vec_pretrained = tp_lstm_word2vec_pretrained / (tp_lstm_word2vec_pretrained + fp_lstm_word2vec_pretrained)\n",
    "accuracy_lstm_word2vec_pretrained = (tp_lstm_word2vec_pretrained + tn_lstm_word2vec_pretrained) / (tn_lstm_word2vec_pretrained + fp_lstm_word2vec_pretrained +fn_lstm_word2vec_pretrained +tp_lstm_word2vec_pretrained)\n",
    "falsePositiveRate_lstm_word2vec_pretrained = fp_lstm_word2vec_pretrained / (fp_lstm_word2vec_pretrained + tn_lstm_word2vec_pretrained)\n",
    "recall_lstm_word2vec_pretrained = tp_lstm_word2vec_pretrained/(tp_lstm_word2vec_pretrained+fn_lstm_word2vec_pretrained)\n",
    "f1_score_lstm_word2vec_pretrained = 2 * ((recall_lstm_word2vec_pretrained*precision_lstm_word2vec_pretrained)/(recall_lstm_word2vec_pretrained+precision_lstm_word2vec_pretrained))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d_lstm_word2vec_pretrained = {'Value' : pd.Series([accuracy_lstm_word2vec_pretrained, precision_lstm_word2vec_pretrained, falsePositiveRate_lstm_word2vec_pretrained, recall_lstm_word2vec_pretrained,f1_score_lstm_word2vec_pretrained], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results_lstm_word2vec_pretrained = pd.DataFrame(d_lstm_word2vec_pretrained) \n",
    "  \n",
    "\n",
    "data_lstm_word2vec_pretrained = {'Negative':[tn_lstm_word2vec_pretrained, fn_lstm_word2vec_pretrained], 'Positive':[fp_lstm_word2vec_pretrained, tp_lstm_word2vec_pretrained]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df_lstm_word2vec_pretrained = pd.DataFrame(data_lstm_word2vec_pretrained, index =['Negative', 'Positive']) \n",
    "df_lstm_word2vec_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.776903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.772340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>0.318452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.852113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.810268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "Accuracy   0.776903\n",
       "Precision  0.772340\n",
       "FPR        0.318452\n",
       "Recall     0.852113\n",
       "F1         0.810268"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lstm_word2vec_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
