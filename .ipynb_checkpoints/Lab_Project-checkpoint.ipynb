{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kELKzweMZaU"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WX4N1L4sMc7z"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROkgT8ELNxwZ"
   },
   "outputs": [],
   "source": [
    "downloaded = drive.CreateFile({'id':'1-Hh3pdJlxhHWy42ZqXFDdjjBxe4vhoRg'})\n",
    "downloaded.GetContentFile('train.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shGyaP_zytVP"
   },
   "source": [
    "# Base Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-xRtolszHr5"
   },
   "source": [
    "## Importing relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isru0U_VzLu2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import pandas as pd\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Dropout, Activation, Embedding\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "# nltk.download()\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEfaIuWN3KT2"
   },
   "source": [
    "## Exploring data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "VfuxUgGt3RAD",
    "outputId": "d5d40978-95d0-42e5-fa33-594c295c2acc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                               Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n",
       "3                                    13,000 people receive #wildfires evacuation orders in California    \n",
       "4             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./nlp-getting-started/train.csv', encoding='utf-8')\n",
    "# Only alter the training variable (#never alter the data variable itself)\n",
    "# training = data\n",
    "# # split the data into train and test set\n",
    "# train, test = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkmCYdOXKPiM"
   },
   "source": [
    "### Analysis\n",
    "- there are 7613 data points\n",
    "- **99.198739%** of the data has **keywords**\n",
    "- **66.73%** of the data has **location** points\n",
    "- the top key word used to extract tweets is **fatalities**\n",
    "- data is ordered in terms of keyword used to extact the tweet from twitter \n",
    "- therefore shuffle the data to mix it.\n",
    "- some of the data contains the # symbol which causes an error when the data is exported onto a numpy array\n",
    "- elements in the **text** column which does not have \" \" marks should not include **,** \n",
    "- data in the **location** column may also include **,** marks which will be read as a column delimeter by **np**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnpG9wg1Kaqe"
   },
   "source": [
    "### Decisions \n",
    "\n",
    "- the most important columns are the text and target columns\n",
    "- the text column contains information about the tweet\n",
    "- the keyword column can be discarded because the keyword appears within the tweet itself.\n",
    "- the location column can be discarded because only 66.73% have a location value associated with them. Droping 33% of the data is impractical\n",
    "- it is however worth exploring whether location of tweet has an impact on the real or fake status of a tweet\n",
    "\n",
    "- in some locations such as a city centre there cannot be a veld fire - so that is a consideration to be made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBgMp8HCJhxe"
   },
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIvSjP1PJfUQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officers evacuation shelter place orders expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding bridge collapse nearby homes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrary thetawniest control wild fires california even northern part state troubling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating ebike collided car little portugal ebike rider suffered serious nonlife thr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest homes razed northern california wildfire abc news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                                                                     text  \\\n",
       "0                                                            deeds reason earthquake may allah forgive us   \n",
       "1                                                                   forest fire near la ronge sask canada   \n",
       "2                residents asked shelter place notified officers evacuation shelter place orders expected   \n",
       "3                                                  people receive wildfires evacuation orders california    \n",
       "4                                                got sent photo ruby alaska smoke wildfires pours school    \n",
       "...                                                                                                   ...   \n",
       "7608                                               two giant cranes holding bridge collapse nearby homes    \n",
       "7609              ariaahrary thetawniest control wild fires california even northern part state troubling   \n",
       "7610                                                                                      volcano hawaii    \n",
       "7611  police investigating ebike collided car little portugal ebike rider suffered serious nonlife thr...   \n",
       "7612                                            latest homes razed northern california wildfire abc news    \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### removing entries where location values are missing\n",
    "data_after_null_removal = data.copy()\n",
    "# data_after_null_removal = data_after_null_removal.dropna(subset=['location'])\n",
    "\n",
    "#punctutation removal\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "data_afer_punctuation_removal = data_after_null_removal.copy()\n",
    "data_afer_punctuation_removal['text'] = data_afer_punctuation_removal['text'].apply(lambda x: clean_text(x))\n",
    "data_afer_punctuation_removal.head(10)\n",
    "\n",
    "# Tockenization\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data_afer_punctuation_removal.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "# stopword removal \n",
    "data_after_stopword_removal = tockenized_data.copy()\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data_after_stopword_removal['text'] = data_after_stopword_removal['text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# return to string\n",
    "data_without_tockenization = data_after_stopword_removal.copy()\n",
    "def listToString(s):     \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "        \n",
    "data_without_tockenization['text'] = data_without_tockenization['text'].apply(lambda x: listToString(x))\n",
    "data_without_tockenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAJ5_I2wLS2w"
   },
   "outputs": [],
   "source": [
    "### train - test split\n",
    "training, testing = train_test_split(data_without_tockenization, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObaL-amMDiEt"
   },
   "outputs": [],
   "source": [
    "train = training.copy()\n",
    "test = testing.copy()\n",
    "# get the dependent and independent variables\n",
    "train_x = train['text']\n",
    "train_y = train['target']\n",
    "test_x = test['text']\n",
    "test_y = test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4JIjdRbvw5f"
   },
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_-YWhL2vzi0"
   },
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "\n",
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 10000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "# Saving the dictionary\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "# padding tockenized text so that it is all the same length(longest word's length).\n",
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "#array of tweets as indeces(words replaced with indexes)\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gewOSOgE0mWW"
   },
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "wYv4KThy0oT6",
    "outputId": "9c6ba703-40cb-4c38-d3d2-5127f21c60ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "386/386 [==============================] - 34s 88ms/step - loss: 0.5299 - accuracy: 0.7388 - precision: 0.7388 - recall: 0.7388 - val_loss: 0.4606 - val_accuracy: 0.7988 - val_precision: 0.7988 - val_recall: 0.7988\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - 33s 85ms/step - loss: 0.2507 - accuracy: 0.9027 - precision: 0.9027 - recall: 0.9027 - val_loss: 0.4903 - val_accuracy: 0.7945 - val_precision: 0.7945 - val_recall: 0.7945\n",
      "Epoch 00002: early stopping\n",
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(512, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','Precision','Recall'])\n",
    "\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "ERhhWVV23Ehl",
    "outputId": "a8521166-3566-4400-e0d7-9d0d1ae38b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdShkFnuJWnN"
   },
   "source": [
    "## Testing Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1faniBdPTZXz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "# for human-friendly printing\n",
    "labels = ['fake', 'real']\n",
    "\n",
    "# read in our saved dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "\n",
    "    return wordIndices\n",
    "\n",
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "f = open(\"output.txt\",\"w\")\n",
    "for index_of_interest, text1 in enumerate(test_x):\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(text1)\n",
    "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    # predict which bucket your input belongs in\n",
    "\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(text1)\n",
    "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        # predict which bucket your input belongs in\n",
    "    pred = model.predict(input)\n",
    "\n",
    "    predictions.append(np.argmax(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mGyqH1Npducg",
    "outputId": "3417db05-3167-43d7-eada-9d28807c725e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>350</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>91</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Positive\n",
       "Negative       350        76\n",
       "Positive        91       245"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = np.array(predictions)\n",
    "test_y = np.array(test_y)\n",
    "confusion_matrix(test_y, predictions)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, predictions).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tn + fp +fn +tp)\n",
    "falsePositiveRate = fp / (fp + tn)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2 * ((recall*precision)/(recall+precision))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d = {'Value' : pd.Series([accuracy, precision, falsePositiveRate, recall,f1_score], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results = pd.DataFrame(d) \n",
    "  \n",
    "\n",
    "data = {'Negative':[tn, fn], 'Positive':[fp, tp]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df = pd.DataFrame(data, index =['Negative', 'Positive']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.780840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.763240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>0.178404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.729167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.745814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "Accuracy   0.780840\n",
       "Precision  0.763240\n",
       "FPR        0.178404\n",
       "Recall     0.729167\n",
       "F1         0.745814"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDpDt1Vw4hmt"
   },
   "source": [
    "### Summary of Base model\n",
    "- The highest achievable accuracy is 77%\n",
    "- The best FPR is 0.09\n",
    "- Model starts overfitting from the first epoch\n",
    "- Validation loss never goes below ~0.45\n",
    "- Attempts were made to vary the batch size. Large batch sizes yield worse performance. The best performance is at 32 or 16\n",
    "- Increasing the number of layers also results in worse performance. Less layers produce better performance\n",
    "- Drop out layers make little to no difference in performance\n",
    "- Including tweets without the keyword seems to yield better performance\n",
    "- Data cleaning produced a slightly less validation loss but not significant enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-trained Word2vec using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data_without_tockenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize data\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data2.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "tockenized_data.head()\n",
    "\n",
    "tweet_data = tockenized_data['text']\n",
    "tweet_data[1]\n",
    "\n",
    "tweet_data_array = []\n",
    "# store tweets in array for word2vec\n",
    "for arr in tweet_data:\n",
    "    tweet_data_array.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training own word2vec model\n",
    "vector_size = 512\n",
    "window_size = 10\n",
    "# Create Word2Vec\n",
    "word2vec = Word2Vec(sentences=tweet_data_array,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "word2vec.save('ownWord2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "word2vec = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f70fc166cd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word vector\n",
    "X_vecs = word2vec.wv\n",
    "\n",
    "X_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 9.545514251937476\n",
      "Max tweet length: 23\n"
     ]
    }
   ],
   "source": [
    "# vector size\n",
    "test_size = int(0.1*len(tweet_data))\n",
    "train_size = len(tweet_data) - test_size\n",
    "\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tweet_data:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tweet_data))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "max_tweet_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "### train - test split\n",
    "training, testing = train_test_split(tockenized_data, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "x_train = training['text']\n",
    "y_train = training['target']\n",
    "\n",
    "x_test = testing['text']\n",
    "y_test = testing['target']\n",
    "\n",
    "# variables for storing word2vec representations of tweets\n",
    "X_train = np.zeros((len(training), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((len(training), 2), dtype=np.int32)\n",
    "X_test = np.zeros((len(testing), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "# converting tweets(training set) to word2vec representations\n",
    "train_y = keras.utils.to_categorical(y_train, 2)\n",
    "t=0\n",
    "for index, tweet in enumerate(x_train):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs:\n",
    "            continue\n",
    "        else:\n",
    "            X_train[index, t, :] = X_vecs[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 15, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweets(testing set) to word2vec representations\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "t=0\n",
    "for index, tweet in enumerate(x_test):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs:\n",
    "            continue\n",
    "        else:\n",
    "            X_test[index, t, :] = X_vecs[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "#             print(test_y['target'] ,'\\n')\n",
    "            if test_y[index] == 1:\n",
    "                 Y_test[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_test[index, :] = [0.0, 1.0] \n",
    "#             Y_test[index, :] = [1.0, 0.0] if test_y[1] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 512)         1326080   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 512)         262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, None, 512)         262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, None, 512)         262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, None, 2)           1026      \n",
      "=================================================================\n",
      "Total params: 2,115,074\n",
      "Trainable params: 788,994\n",
      "Non-trainable params: 1,326,080\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 2) and (None, 10000, 2) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f2b8d3086410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m history = model.fit(train_x, train_y,\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 2) and (None, 10000, 2) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Creating a model\n",
    "embedding_features = 16\n",
    "model = Sequential()\n",
    "model.add(word2vec.wv.get_keras_embedding(train_embeddings=False))\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('LSTM1_EmbeddingModel.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('LSTM1_EmbeddingModel.h5')\n",
    "\n",
    "print('saved model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training own Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data_without_tockenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data2.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "tockenized_data.head()\n",
    "\n",
    "tweet_data = tockenized_data['text']\n",
    "tweet_data[1]\n",
    "\n",
    "tweet_data_array = []\n",
    "\n",
    "for arr in tweet_data:\n",
    "    tweet_data_array.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 512\n",
    "window_size = 10\n",
    "# Create Word2Vec\n",
    "word2vec_self_trained = Word2Vec(sentences=tweet_data_array,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_self_trained.save('word2vecSelfTrained.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_self_trained = Word2Vec.load('word2vecSelfTrained.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vecs_self_trained = word2vec_self_trained.wv\n",
    "\n",
    "X_vecs_self_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.1*len(tweet_data))\n",
    "train_size = len(tweet_data) - test_size\n",
    "\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tweet_data:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tweet_data))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "max_tweet_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "### train - test split\n",
    "training, testing = train_test_split(tockenized_data, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "training\n",
    "\n",
    "x_train = training['text']\n",
    "y_train = training['target']\n",
    "\n",
    "x_test = testing['text']\n",
    "y_test = testing['target']\n",
    "\n",
    "\n",
    "X_train = np.zeros((len(training), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((len(training), 2), dtype=np.int32)\n",
    "X_test = np.zeros((len(testing), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "train_y = np.array(y_train)\n",
    "\n",
    "t=0\n",
    "for index, tweet in enumerate(x_train):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs_self_trained:\n",
    "            continue\n",
    "        else:\n",
    "            X_train[index, t, :] = X_vecs_self_trained[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "                \n",
    "            if train_y[index] == 1:\n",
    "                 Y_train[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_train[index, :] = [0.0, 1.0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "for index, tweet in enumerate(x_test):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs_self_trained:\n",
    "            continue\n",
    "        else:\n",
    "            X_test[index, t, :] = X_vecs_self_trained[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "#             print(test_y['target'] ,'\\n')\n",
    "            if test_y[index] == 1:\n",
    "                 Y_test[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_test[index, :] = [0.0, 1.0] \n",
    "#             Y_test[index, :] = [1.0, 0.0] if test_y[1] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = keras.utils.to_categorical(y_train, 2)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','Precision','Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,Y_test,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lstm_word2vec_self_trained = []\n",
    "\n",
    "for ind, lab in Y_test:\n",
    "    if Y_test[ind][0] == 0:\n",
    "        y_test_lstm_word2vec_self_trained.append(0)\n",
    "    else:\n",
    "        y_test_lstm_word2vec_self_trained.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "Y_pred = model.predict_generator(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "tn_lstm_word2vec_self_trained, fp_lstm_word2vec_self_trained, fn_lstm_word2vec_self_trained, tp_lstm_word2vec_self_trained = confusion_matrix(y_test_lstm_word2vec_self_trained, y_pred).ravel()\n",
    "precision_lstm_word2vec_self_trained = tp_lstm_word2vec_self_trained / (tp_lstm_word2vec_self_trained + fp_lstm_word2vec_self_trained)\n",
    "accuracy_lstm_word2vec_self_trained = (tp_lstm_word2vec_self_trained + tn_lstm_word2vec_self_trained) / (tn_lstm_word2vec_self_trained + fp_lstm_word2vec_self_trained +fn_lstm_word2vec_self_trained +tp_lstm_word2vec_self_trained)\n",
    "falsePositiveRate_lstm_word2vec_self_trained = fp_lstm_word2vec_self_trained / (fp_lstm_word2vec_self_trained + tn_lstm_word2vec_self_trained)\n",
    "recall_lstm_word2vec_self_trained = tp_lstm_word2vec_self_trained/(tp_lstm_word2vec_self_trained+fn_lstm_word2vec_self_trained)\n",
    "f1_score_lstm_word2vec_self_trained = 2 * ((recall_lstm_word2vec_self_trained*precision_lstm_word2vec_self_trained)/(recall_lstm_word2vec_self_trained+precision_lstm_word2vec_self_trained))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d = {'Value' : pd.Series([accuracy_lstm_word2vec_self_trained, precision_lstm_word2vec_self_trained, falsePositiveRate_lstm_word2vec_self_trained, recall_lstm_word2vec_self_trained,f1_score_lstm_word2vec_self_trained], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results_lstm_word2vec_self_trained = pd.DataFrame(d) \n",
    "  \n",
    "\n",
    "data = {'Negative':[tn_lstm_word2vec_self_trained, fn_lstm_word2vec_self_trained], 'Positive':[fp_lstm_word2vec_self_trained, tp_lstm_word2vec_self_trained]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df_lstm_word2vec_self_trained = pd.DataFrame(data, index =['Negative', 'Positive']) \n",
    "df_lstm_word2vec_self_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lstm_word2vec_self_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "pretrainedModel = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec.save('pretrainedModel.model')\n",
    "word2vec_pretrained = Word2Vec.load('pretrainedModel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fd7c3cf53a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vecs_pretrained = word2vec_pretrained.wv\n",
    "\n",
    "X_vecs_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 9.545514251937476\n",
      "Max tweet length: 23\n"
     ]
    }
   ],
   "source": [
    "test_size = int(0.1*len(tweet_data))\n",
    "train_size = len(tweet_data) - test_size\n",
    "\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tweet_data:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tweet_data))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "max_tweet_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "### train - test split\n",
    "training, testing = train_test_split(tockenized_data, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "training\n",
    "\n",
    "x_train = training['text']\n",
    "y_train = training['target']\n",
    "\n",
    "x_test = testing['text']\n",
    "y_test = testing['target']\n",
    "\n",
    "\n",
    "X_train = np.zeros((len(training), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((len(training), 2), dtype=np.int32)\n",
    "X_test = np.zeros((len(testing), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "train_y = np.array(y_train)\n",
    "\n",
    "t=0\n",
    "for index, tweet in enumerate(x_train):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs_pretrained:\n",
    "            continue\n",
    "        else:\n",
    "            X_train[index, t, :] = X_vecs_pretrained[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "                \n",
    "            if train_y[index] == 1:\n",
    "                 Y_train[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_train[index, :] = [0.0, 1.0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(y_test)\n",
    "t=0\n",
    "for index, tweet in enumerate(x_test):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs_pretrained:\n",
    "            continue\n",
    "        else:\n",
    "            X_test[index, t, :] = X_vecs_pretrained[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "#             print(test_y['target'] ,'\\n')\n",
    "            if test_y[index] == 1:\n",
    "                 Y_test[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_test[index, :] = [0.0, 1.0] \n",
    "#             Y_test[index, :] = [1.0, 0.0] if test_y[1] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = keras.utils.to_categorical(y_train, 2)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','Precision','Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input Tensor(\"embedding_3_input:0\", shape=(None, None), dtype=float32), but it was called on an input with incompatible shape (None, 15, 512).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:277 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:717 call\n        return self._run_internal_graph(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:888 _run_internal_graph\n        output_tensors = layer(computed_tensors, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:654 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:176 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer lstm_2 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 15, 512, 512]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-17f895f1798c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(X_train, Y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:277 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:717 call\n        return self._run_internal_graph(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:888 _run_internal_graph\n        output_tensors = layer(computed_tensors, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:654 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:176 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer lstm_2 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 15, 512, 512]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,Y_test,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lstm_word2vec_pretrained = []\n",
    "\n",
    "for ind, lab in Y_test:\n",
    "    if Y_test[ind][0] == 0:\n",
    "        y_test_lstm_word2vec_pretrained.append(0)\n",
    "    else:\n",
    "        y_test_lstm_word2vec_pretrained.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "Y_pred = model.predict_generator(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "tn_lstm_word2vec_pretrained, fp_lstm_word2vec_pretrained, fn_lstm_word2vec_pretrained, tp_lstm_word2vec_pretrained = confusion_matrix(y_test_lstm_word2vec_pretrained, y_pred).ravel()\n",
    "precision_lstm_word2vec_pretrained = tp_lstm_word2vec_pretrained / (tp_lstm_word2vec_pretrained + fp_lstm_word2vec_pretrained)\n",
    "accuracy_lstm_word2vec_pretrained = (tp_lstm_word2vec_pretrained + tn_lstm_word2vec_pretrained) / (tn_lstm_word2vec_pretrained + fp_lstm_word2vec_pretrained +fn_lstm_word2vec_pretrained +tp_lstm_word2vec_pretrained)\n",
    "falsePositiveRate_lstm_word2vec_pretrained = fp_lstm_word2vec_pretrained / (fp_lstm_word2vec_pretrained + tn_lstm_word2vec_pretrained)\n",
    "recall_lstm_word2vec_pretrained = tp_lstm_word2vec_pretrained/(tp_lstm_word2vec_pretrained+fn_lstm_word2vec_pretrained)\n",
    "f1_score_lstm_word2vec_pretrained = 2 * ((recall_lstm_word2vec_pretrained*precision_lstm_word2vec_pretrained)/(recall_lstm_word2vec_pretrained+precision_lstm_word2vec_pretrained))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d_lstm_word2vec_pretrained = {'Value' : pd.Series([accuracy_lstm_word2vec_pretrained, precision_lstm_word2vec_pretrained, falsePositiveRate_lstm_word2vec_pretrained, recall_lstm_word2vec_pretrained,f1_score_lstm_word2vec_pretrained], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results_lstm_word2vec_pretrained = pd.DataFrame(d_lstm_word2vec_pretrained) \n",
    "  \n",
    "\n",
    "data_lstm_word2vec_pretrained = {'Negative':[tn_lstm_word2vec_pretrained, fn_lstm_word2vec_pretrained], 'Positive':[fp_lstm_word2vec_pretrained, tp_lstm_word2vec_pretrained]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df_lstm_word2vec_pretrained = pd.DataFrame(data_lstm_word2vec_pretrained, index =['Negative', 'Positive']) \n",
    "df_lstm_word2vec_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lstm_word2vec_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officers evacuation shelter place orders expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                       text  \\\n",
       "0                                              deeds reason earthquake may allah forgive us   \n",
       "1                                                     forest fire near la ronge sask canada   \n",
       "2  residents asked shelter place notified officers evacuation shelter place orders expected   \n",
       "3                                    people receive wildfires evacuation orders california    \n",
       "4                                  got sent photo ruby alaska smoke wildfires pours school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data3 = data_without_tockenization.copy()\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train - test split\n",
    "# training, testing = train_test_split(data3, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = data3.loc[:6852,'text'].values\n",
    "y_train = data3.loc[:6852,'target'].values\n",
    "X_test =  data3.loc[6853:,'text'].values\n",
    "y_test = data3.loc[6853:,'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tensorflow.python.keras.preprocessing import Tokenizer\n",
    "# # from tensorflow.python.keras.preprocessing import pad_sequences\n",
    "\n",
    "# tokenizer_obj = Tokenizer()\n",
    "# total_tweets =  X_train + X_test\n",
    "# tokenizer_obj.fit_on_texts(total_tweets)\n",
    "\n",
    "# # pad sequences\n",
    "# max_length = max([len(s.split()) for s in total_tweets])\n",
    "\n",
    "# # define vocab size\n",
    "# vocab_size = len(tokenizer_obj.word_index) +1\n",
    "\n",
    "# x_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\n",
    "# X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
    "\n",
    "# X_train_pad = pad_sequences(X_train_tokens,maxlen=max_length,padding='post')\n",
    "# X_test_pad = pad_sequences(X_test_tokens,maxlen=max_length, paddding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_lines = list()\n",
    "lines = data3['text'].values.tolist()\n",
    "\n",
    "for line in lines:\n",
    "    tokens = word_tokenize(line)\n",
    "    #convert to lower case \n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remainining tokens that are not alphanumeric\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    #filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words  = [w for w in words if not w in stop_words]\n",
    "    tweet_lines.append(words)\n",
    "    \n",
    "len(tweet_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_test_lines = list()\n",
    "# test_lines = testing['text'].values.tolist()\n",
    "\n",
    "# for line in test_lines:\n",
    "#     tokens = word_tokenize(line)\n",
    "#     #convert to lower case \n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "#     # remove punctuation\n",
    "#     table = str.maketrans('','', string.punctuation)\n",
    "#     stripped = [w.translate(table) for w in tokens]\n",
    "#     # remove remainining tokens that are not alphanumeric\n",
    "#     words = [word for word in stripped if word.isalpha()]\n",
    "#     #filter out stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     words  = [w for w in words if not w in stop_words]\n",
    "#     tweet_test_lines.append(words)\n",
    "    \n",
    "# len(tweet_test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 16430\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "#train word2vec model\n",
    "new_model = gensim.models.Word2Vec(sentences=tweet_lines, size=100, window=5,workers=3,min_count=1)\n",
    "#vocabsize \n",
    "words = list(new_model.wv.vocab)\n",
    "print('Vocab size', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model \n",
    "filename = 'new_word2vec_model.txt'\n",
    "new_model.wv.save_word2vec_format(filename,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embedding_index = {}\n",
    "f = open(os.path.join('','new_word2vec_model.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embedding_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16430 uniique tokens.\n",
      "Shape of review tensor: (7613, 7613)\n",
      "Shape of tweet tensor (7613,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(tweet_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(tweet_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s uniique tokens.' % len(word_index))\n",
    "\n",
    "\n",
    "max_length = 6851 + 762\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "tweet = data3['target'].values\n",
    "print('Shape of review tensor:',review_pad.shape)\n",
    "print('Shape of tweet tensor', tweet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16431\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM =100\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #words not found in embedding index will all be zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-cce080856f95>:6: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  pretrained_weights = new_model.wv.syn0\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.layers import GRU\n",
    "\n",
    "\n",
    "\n",
    "pretrained_weights = new_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "\n",
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training['text'].values\n",
    "y_train = training['target'].values\n",
    "\n",
    "len(tweet_lines)\n",
    "VALIDATION_SPLIT=0.1\n",
    "\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "tweet = tweet[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT*review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = tweet[:-num_validation_samples]\n",
    "X_test = review_pad[-num_validation_samples:]\n",
    "y_test = tweet[-num_validation_samples:]\n",
    "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "# y_train = keras.utils.to_categorical(y_train, 2)\n",
    "# y_test = keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[121808,16430] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[node sequential_8/dense_11/Tensordot/MatMul (defined at <ipython-input-80-1f95a306ceb1>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_7456]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-1f95a306ceb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(X_train_pad, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[121808,16430] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[node sequential_8/dense_11/Tensordot/MatMul (defined at <ipython-input-80-1f95a306ceb1>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_7456]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test,y_test),\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
