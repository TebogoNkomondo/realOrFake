{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kELKzweMZaU"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WX4N1L4sMc7z"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROkgT8ELNxwZ"
   },
   "outputs": [],
   "source": [
    "downloaded = drive.CreateFile({'id':'1-Hh3pdJlxhHWy42ZqXFDdjjBxe4vhoRg'})\n",
    "downloaded.GetContentFile('train.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shGyaP_zytVP"
   },
   "source": [
    "# Base Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-xRtolszHr5"
   },
   "source": [
    "## Importing relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isru0U_VzLu2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import pandas as pd\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, Dropout, Activation, Embedding\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "# nltk.download()\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEfaIuWN3KT2"
   },
   "source": [
    "## Exploring data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "VfuxUgGt3RAD",
    "outputId": "d5d40978-95d0-42e5-fa33-594c295c2acc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0   1     NaN      NaN   \n",
       "1   4     NaN      NaN   \n",
       "2   5     NaN      NaN   \n",
       "3   6     NaN      NaN   \n",
       "4   7     NaN      NaN   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0                                Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                               Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n",
       "3                                    13,000 people receive #wildfires evacuation orders in California    \n",
       "4             Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./nlp-getting-started/train.csv')\n",
    "# Only alter the training variable (#never alter the data variable itself)\n",
    "# training = data\n",
    "# # split the data into train and test set\n",
    "# train, test = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkmCYdOXKPiM"
   },
   "source": [
    "### Analysis\n",
    "- there are 7613 data points\n",
    "- **99.198739%** of the data has **keywords**\n",
    "- **66.73%** of the data has **location** points\n",
    "- the top key word used to extract tweets is **fatalities**\n",
    "- data is ordered in terms of keyword used to extact the tweet from twitter \n",
    "- therefore shuffle the data to mix it.\n",
    "- some of the data contains the # symbol which causes an error when the data is exported onto a numpy array\n",
    "- elements in the **text** column which does not have \" \" marks should not include **,** \n",
    "- data in the **location** column may also include **,** marks which will be read as a column delimeter by **np**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnpG9wg1Kaqe"
   },
   "source": [
    "### Decisions \n",
    "\n",
    "- the most important columns are the text and target columns\n",
    "- the text column contains information about the tweet\n",
    "- the keyword column can be discarded because the keyword appears within the tweet itself.\n",
    "- the location column can be discarded because only 66.73% have a location value associated with them. Droping 33% of the data is impractical\n",
    "- it is however worth exploring whether location of tweet has an impact on the real or fake status of a tweet\n",
    "\n",
    "- in some locations such as a city centre there cannot be a veld fire - so that is a consideration to be made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBgMp8HCJhxe"
   },
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VIvSjP1PJfUQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>residents asked shelter place notified officers evacuation shelter place orders expected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding bridge collapse nearby homes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariaahrary thetawniest control wild fires california even northern part state troubling</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating ebike collided car little portugal ebike rider suffered serious nonlife thr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest homes razed northern california wildfire abc news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                                                                     text  \\\n",
       "0                                                            deeds reason earthquake may allah forgive us   \n",
       "1                                                                   forest fire near la ronge sask canada   \n",
       "2                residents asked shelter place notified officers evacuation shelter place orders expected   \n",
       "3                                                  people receive wildfires evacuation orders california    \n",
       "4                                                got sent photo ruby alaska smoke wildfires pours school    \n",
       "...                                                                                                   ...   \n",
       "7608                                               two giant cranes holding bridge collapse nearby homes    \n",
       "7609              ariaahrary thetawniest control wild fires california even northern part state troubling   \n",
       "7610                                                                                      volcano hawaii    \n",
       "7611  police investigating ebike collided car little portugal ebike rider suffered serious nonlife thr...   \n",
       "7612                                            latest homes razed northern california wildfire abc news    \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### removing entries where location values are missing\n",
    "data_after_null_removal = data.copy()\n",
    "# data_after_null_removal = data_after_null_removal.dropna(subset=['location'])\n",
    "\n",
    "#punctutation removal\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "data_afer_punctuation_removal = data_after_null_removal.copy()\n",
    "data_afer_punctuation_removal['text'] = data_afer_punctuation_removal['text'].apply(lambda x: clean_text(x))\n",
    "data_afer_punctuation_removal.head(10)\n",
    "\n",
    "# Tockenization\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data_afer_punctuation_removal.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "# stopword removal \n",
    "data_after_stopword_removal = tockenized_data.copy()\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data_after_stopword_removal['text'] = data_after_stopword_removal['text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# return to string\n",
    "data_without_tockenization = data_after_stopword_removal.copy()\n",
    "def listToString(s):     \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    # return string   \n",
    "    return (str1.join(s)) \n",
    "        \n",
    "data_without_tockenization['text'] = data_without_tockenization['text'].apply(lambda x: listToString(x))\n",
    "data_without_tockenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAJ5_I2wLS2w"
   },
   "outputs": [],
   "source": [
    "### train - test split\n",
    "training, testing = train_test_split(data_without_tockenization, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObaL-amMDiEt"
   },
   "outputs": [],
   "source": [
    "train = training.copy()\n",
    "test = testing.copy()\n",
    "# get the dependent and independent variables\n",
    "train_x = train['text']\n",
    "train_y = train['target']\n",
    "test_x = test['text']\n",
    "test_y = test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4JIjdRbvw5f"
   },
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_-YWhL2vzi0"
   },
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "\n",
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 10000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "\n",
    "# Saving the dictionary\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "# padding tockenized text so that it is all the same length(longest word's length).\n",
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "#array of tweets as indeces(words replaced with indexes)\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gewOSOgE0mWW"
   },
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "wYv4KThy0oT6",
    "outputId": "9c6ba703-40cb-4c38-d3d2-5127f21c60ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "386/386 [==============================] - 30s 78ms/step - loss: 0.5392 - accuracy: 0.7379 - precision: 0.7379 - recall: 0.7379 - val_loss: 0.4883 - val_accuracy: 0.7770 - val_precision: 0.7770 - val_recall: 0.7770\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - 30s 77ms/step - loss: 0.2549 - accuracy: 0.9025 - precision: 0.9025 - recall: 0.9025 - val_loss: 0.5086 - val_accuracy: 0.7974 - val_precision: 0.7974 - val_recall: 0.7974\n",
      "Epoch 00002: early stopping\n",
      "saved model!\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(512, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','Precision','Recall'])\n",
    "\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "ERhhWVV23Ehl",
    "outputId": "a8521166-3566-4400-e0d7-9d0d1ae38b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_precision', 'val_recall'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training and Validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdShkFnuJWnN"
   },
   "source": [
    "## Testing Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1faniBdPTZXz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "# for human-friendly printing\n",
    "labels = ['fake', 'real']\n",
    "\n",
    "# read in our saved dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "\n",
    "    return wordIndices\n",
    "\n",
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "f = open(\"output.txt\",\"w\")\n",
    "for index_of_interest, text1 in enumerate(test_x):\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(text1)\n",
    "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    # predict which bucket your input belongs in\n",
    "\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(text1)\n",
    "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        # predict which bucket your input belongs in\n",
    "    pred = model.predict(input)\n",
    "\n",
    "    predictions.append(np.argmax(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mGyqH1Npducg",
    "outputId": "3417db05-3167-43d7-eada-9d28807c725e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>378</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>121</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Positive\n",
       "Negative       378        48\n",
       "Positive       121       215"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = np.array(predictions)\n",
    "test_y = np.array(test_y)\n",
    "confusion_matrix(test_y, predictions)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, predictions).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tn + fp +fn +tp)\n",
    "falsePositiveRate = fp / (fp + tn)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2 * ((recall*precision)/(recall+precision))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d = {'Value' : pd.Series([accuracy, precision, falsePositiveRate, recall,f1_score], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results = pd.DataFrame(d) \n",
    "  \n",
    "\n",
    "data = {'Negative':[tn, fn], 'Positive':[fp, tp]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df = pd.DataFrame(data, index =['Negative', 'Positive']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.778215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.817490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>0.112676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.639881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.717863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "Accuracy   0.778215\n",
       "Precision  0.817490\n",
       "FPR        0.112676\n",
       "Recall     0.639881\n",
       "F1         0.717863"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDpDt1Vw4hmt"
   },
   "source": [
    "### Summary of Base model\n",
    "- The highest achievable accuracy is 77%\n",
    "- The best FPR is 0.09\n",
    "- Model starts overfitting from the first epoch\n",
    "- Validation loss never goes below ~0.45\n",
    "- Attempts were made to vary the batch size. Large batch sizes yield worse performance. The best performance is at 32 or 16\n",
    "- Increasing the number of layers also results in worse performance. Less layers produce better performance\n",
    "- Drop out layers make little to no difference in performance\n",
    "- Including tweets without the keyword seems to yield better performance\n",
    "- Data cleaning produced a slightly less validation loss but not significant enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-trained Word2vec using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data_without_tockenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize data\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data2.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "tockenized_data.head()\n",
    "\n",
    "tweet_data = tockenized_data['text']\n",
    "tweet_data[1]\n",
    "\n",
    "tweet_data_array = []\n",
    "# store tweets in array for word2vec\n",
    "for arr in tweet_data:\n",
    "    tweet_data_array.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training own word2vec model\n",
    "vector_size = 512\n",
    "window_size = 10\n",
    "# Create Word2Vec\n",
    "word2vec = Word2Vec(sentences=tweet_data_array,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "word2vec.save('ownWord2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "word2vec = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f9139f5a310>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word vector\n",
    "X_vecs = word2vec.wv\n",
    "\n",
    "X_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 9.921318796794957\n",
      "Max tweet length: 23\n"
     ]
    }
   ],
   "source": [
    "# vector size\n",
    "test_size = int(0.1*len(tweet_data))\n",
    "train_size = len(tweet_data) - test_size\n",
    "\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tweet_data:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tweet_data))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "max_tweet_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "### train - test split\n",
    "training, testing = train_test_split(tockenized_data, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "x_train = training['text']\n",
    "y_train = training['target']\n",
    "\n",
    "x_test = testing['text']\n",
    "y_test = testing['target']\n",
    "\n",
    "# variables for storing word2vec representations of tweets\n",
    "X_train = np.zeros((len(training), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((len(training), 2), dtype=np.int32)\n",
    "X_test = np.zeros((len(testing), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "# converting tweets(training set) to word2vec representations\n",
    "train_y = keras.utils.to_categorical(y_train, 2)\n",
    "t=0\n",
    "for index, tweet in enumerate(x_train):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs:\n",
    "            continue\n",
    "        else:\n",
    "            X_train[index, t, :] = X_vecs[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 15, 512)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweets(testing set) to word2vec representations\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "t=0\n",
    "for index, tweet in enumerate(x_test):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs:\n",
    "            continue\n",
    "        else:\n",
    "            X_test[index, t, :] = X_vecs[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "#             print(test_y['target'] ,'\\n')\n",
    "            if test_y[index] == 1:\n",
    "                 Y_test[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_test[index, :] = [0.0, 1.0] \n",
    "#             Y_test[index, :] = [1.0, 0.0] if test_y[1] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_18 is incompatible with the layer: expected axis -1 of input shape to have value 10000 but received input with shape [None, 15, 512]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-573faad62ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m history = model.fit(X_train, Y_train,\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    /home/tebogo/anaconda3/envs/py3-TF2.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_18 is incompatible with the layer: expected axis -1 of input shape to have value 10000 but received input with shape [None, 15, 512]\n"
     ]
    }
   ],
   "source": [
    "# using the base model's architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy','Precision','Recall'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')\n",
    "\n",
    "print('saved model!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training own Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data_without_tockenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tockenized_data = data2.copy()\n",
    "tockenized_data['text'] = tockenized_data['text'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "tockenized_data.head()\n",
    "\n",
    "tweet_data = tockenized_data['text']\n",
    "tweet_data[1]\n",
    "\n",
    "tweet_data_array = []\n",
    "\n",
    "for arr in tweet_data:\n",
    "    tweet_data_array.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 512\n",
    "window_size = 10\n",
    "# Create Word2Vec\n",
    "word2vec = Word2Vec(sentences=tweet_data_array,\n",
    "                    size=vector_size, \n",
    "                    window=window_size, \n",
    "                    negative=20,\n",
    "                    iter=50,\n",
    "                    seed=1000,\n",
    "                    workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f5690650e20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vecs = word2vec.wv\n",
    "\n",
    "X_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tweet length: 9.921318796794957\n",
      "Max tweet length: 23\n"
     ]
    }
   ],
   "source": [
    "test_size = int(0.1*len(tweet_data))\n",
    "train_size = len(tweet_data) - test_size\n",
    "\n",
    "avg_length = 0.0\n",
    "max_length = 0\n",
    "\n",
    "for tweet in tweet_data:\n",
    "    if len(tweet) > max_length:\n",
    "        max_length = len(tweet)\n",
    "    avg_length += float(len(tweet))\n",
    "    \n",
    "print('Average tweet length: {}'.format(avg_length / float(len(tweet_data))))\n",
    "print('Max tweet length: {}'.format(max_length))\n",
    "\n",
    "max_tweet_length = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "### train - test split\n",
    "training, testing = train_test_split(tockenized_data, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "training\n",
    "\n",
    "x_train = training['text']\n",
    "y_train = training['target']\n",
    "\n",
    "x_test = testing['text']\n",
    "y_test = testing['target']\n",
    "\n",
    "\n",
    "X_train = np.zeros((len(training), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_train = np.zeros((len(training), 2), dtype=np.int32)\n",
    "X_test = np.zeros((len(testing), max_tweet_length, vector_size), dtype=K.floatx())\n",
    "Y_test = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "train_y = np.array(y_train)\n",
    "\n",
    "t=0\n",
    "for index, tweet in enumerate(x_train):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs:\n",
    "            continue\n",
    "        else:\n",
    "            X_train[index, t, :] = X_vecs[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "                \n",
    "            if train_y[index] == 1:\n",
    "                 Y_train[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_train[index, :] = [0.0, 1.0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "for index, tweet in enumerate(x_test):\n",
    "    for word in tweet:\n",
    "        if word not in X_vecs:\n",
    "            continue\n",
    "        else:\n",
    "            X_test[index, t, :] = X_vecs[word]\n",
    "            t+=1\n",
    "            \n",
    "            if t == max_tweet_length:\n",
    "                t=0\n",
    "#             print(test_y['target'] ,'\\n')\n",
    "            if test_y[index] == 1:\n",
    "                 Y_test[index, :] = [1.0, 0.0] \n",
    "            else:\n",
    "                Y_test[index, :] = [0.0, 1.0] \n",
    "#             Y_test[index, :] = [1.0, 0.0] if test_y[1] == 0 else [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = keras.utils.to_categorical(y_train, 2)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','Precision','Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "386/386 [==============================] - 57s 148ms/step - loss: 0.5193 - accuracy: 0.7591 - precision: 0.7591 - recall: 0.7611 - val_loss: 0.5110 - val_accuracy: 0.7697 - val_precision: 0.7697 - val_recall: 0.7697\n",
      "Epoch 2/10\n",
      "386/386 [==============================] - 55s 143ms/step - loss: 0.4642 - accuracy: 0.7940 - precision: 0.7940 - recall: 0.7961 - val_loss: 0.4785 - val_accuracy: 0.7828 - val_precision: 0.7828 - val_recall: 0.7828\n",
      "Epoch 3/10\n",
      "386/386 [==============================] - 56s 144ms/step - loss: 0.4305 - accuracy: 0.8071 - precision: 0.8070 - recall: 0.8091 - val_loss: 0.4787 - val_accuracy: 0.7886 - val_precision: 0.7886 - val_recall: 0.7886\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5690d49be0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "    batch_size=16,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 15, 512)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 2s 45ms/step - loss: 0.5277 - accuracy: 0.7651 - precision: 0.7651 - recall: 0.7671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.527721107006073, 0.7650918364524841, 0.7650918364524841, 0.7671052813529968]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "y_test_final = np.zeros((len(testing), 2), dtype=np.int32)\n",
    "\n",
    "for ind, lab in Y_test:\n",
    "    print(lab)\n",
    "#     if lab[0] == 1:\n",
    "#         y_test_final[ind] = 1\n",
    "#     else:\n",
    "#         y_test_final[ind] = 0\n",
    "\n",
    "# y_test_final        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>54</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>213</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Negative  Positive\n",
       "Negative        54       372\n",
       "Positive       213       123"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "Y_pred = model.predict_generator(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, y_pred).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tn + fp +fn +tp)\n",
    "falsePositiveRate = fp / (fp + tn)\n",
    "recall = tp/(tp+fn)\n",
    "f1_score = 2 * ((recall*precision)/(recall+precision))\n",
    "\n",
    "# Intialise data to Dicts of series. \n",
    "d = {'Value' : pd.Series([accuracy, precision, falsePositiveRate, recall,f1_score], index =['Accuracy', 'Precision', 'FPR','Recall','F1'])} \n",
    "  \n",
    "# creates Dataframe. \n",
    "results = pd.DataFrame(d) \n",
    "  \n",
    "\n",
    "data = {'Negative':[tn, fn], 'Positive':[fp, tp]} \n",
    "  \n",
    "# Creates pandas DataFrame. \n",
    "df = pd.DataFrame(data, index =['Negative', 'Positive']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.232283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.248485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>0.873239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.366071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.296029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "Accuracy   0.232283\n",
       "Precision  0.248485\n",
       "FPR        0.873239\n",
       "Recall     0.366071\n",
       "F1         0.296029"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
